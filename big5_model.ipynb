{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "* run 1 - tf-idf features with ridge regression (alpha = 0.5), all r-squared values negative\n",
    "* run 2 - log-counts with ridge regression (alpha = 0.5), results are now positive but still low\n",
    "* run 3 - log-counts with random forest (max_depth=2, random_state=0), results are worse \n",
    "* run 4 - log-counts with random forest (max_depth=2), results are yet worse \n",
    "* run 5 - log-counts with random forest (max_depth=5, random_state=0), still bad\n",
    "* run 6 - log-counts with ridge regression (alpha = 0.75), no noticeable difference from run 2\n",
    "* run 7 - log-counts with ridge regression (alpha = 0.25), no noticeable difference from run 2\n",
    "* run 8 - log-counts with kernel ridge regression (kernel=\"rbf\", gamma=0.1, alpha=0.5), hideous performance\n",
    "* run 9 - log-counts with SVR with defaults, all r-squared values negative\n",
    "* run 10 - log-counts with SVR (kernel=\"linear\"), similar to run 2\n",
    "* run 11 - log-counts with SVR (kernel=\"poly\"), maybe a slight improvement but not enough to be useful\n",
    "* run 12 - log-counts with lasso regression (alpha = 0.5), all r-squared values are negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent_string = 'None' #replace with your user agent string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def get_comment_text(r):\n",
    "    comments = []\n",
    "    cid = None\n",
    "    \n",
    "    for comment in r['data']['children']:\n",
    "        body = comment['data']['body']\n",
    "        cid = comment['data']['name'] #after id\n",
    "        comments.append(body)\n",
    "        \n",
    "    return comments, cid\n",
    "\n",
    "def load_csv(filepath):\n",
    "    return [line.strip().split(',') for line in open(filepath, \"r\")]\n",
    "\n",
    "punc_list = str.maketrans({\"\\'\":\" \", \"\\\"\":\" \", \",\":\" \", \".\":\" \", \"?\":\" \", \"!\":\" \", \"-\":\" \", \"/\":\" \", \";\":\" \", \":\":\" \", \"{\":\" \", \"}\":\" \", \"_\":\" \",\"*\":\" \", \"^\":\" \", \"~\":\" \", \"`\":\" \"})\n",
    "\n",
    "def tokenize(string):\n",
    "    string = string.strip().lower()\n",
    "    string = string.translate(punc_list) #depunctuate\n",
    "    string_list = re.split(\"\\s+\", string)\n",
    "    string_list = [stemmer.stem(token) for token in string_list]\n",
    "    return string_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify usernames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_csv(\"big5.csv\")\n",
    "\n",
    "#get list of names\n",
    "names = [row[0] for row in data[1:]]\n",
    "\n",
    "for name in names:\n",
    "    \n",
    "    r = requests.get(\"https://www.reddit.com/user/{}/comments.json?limit=1\".format(name), headers = {'User-agent':user_agent_string})\n",
    "    r_data = r.json()\n",
    "\n",
    "    if 'error' in r_data.keys():\n",
    "        print(r_data)\n",
    "        print(name)\n",
    "        \n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape user posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time, os\n",
    "\n",
    "def get_comment_text(r):\n",
    "    comments = []\n",
    "    cid = None\n",
    "    \n",
    "    for comment in r['data']['children']:\n",
    "        body = comment['data']['body']\n",
    "        cid = comment['data']['name'] #after id\n",
    "        comments.append(body)\n",
    "        \n",
    "    return comments, cid\n",
    "\n",
    "def load_csv(filepath):\n",
    "    return [line.strip().split(',') for line in open(filepath, \"r\")]\n",
    "\n",
    "#open big5.csv\n",
    "data = load_csv(\"big5.csv\")\n",
    "\n",
    "#get list of names\n",
    "names = [row[0] for row in data[1:]]\n",
    "#filterlist = [ID[:-4] for ID in os.listdir(\"comments\")]\n",
    "#names = [name for name in names if name not in filterlist]\n",
    "#print(names)\n",
    "    \n",
    "for name in names:\n",
    "    \n",
    "    last_id = None\n",
    "    comment_text = None\n",
    "    \n",
    "    while comment_text != []:\n",
    "        if last_id == None:\n",
    "            r = requests.get(\"https://www.reddit.com/user/{}/comments.json?limit=100\".format(name), headers = {'User-agent':user_agent_string})\n",
    "        else:\n",
    "            r = requests.get(\"https://www.reddit.com/user/{}/comments.json?limit=100&after={}\".format(name, last_id), headers = {'User-agent':user_agent_string})\n",
    "\n",
    "        r_data = r.json()\n",
    "        \n",
    "        if 'error' in r_data.keys() and r_data['error'] == 403:\n",
    "            break\n",
    "        \n",
    "        till_reset = int(r.headers['x-ratelimit-reset'])\n",
    "        req_remaining = int(r.headers['x-ratelimit-remaining'])\n",
    "        \n",
    "        if req_remaining == 0:\n",
    "            time.sleep(till_reset+5)\n",
    "        \n",
    "        with open(\"comments/{}.txt\".format(name), \"a\") as fh:\n",
    "            comment_text, last_id = get_comment_text(r_data)\n",
    "            \n",
    "            for comment in comment_text:\n",
    "                comment = comment.encode(\"ascii\", errors=\"ignore\").decode()\n",
    "                fh.write(comment+\"\\n\")\n",
    "                fh.write(\"*BREAK*\\n\")\n",
    "\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os, math\n",
    "\n",
    "class WordFeatures:\n",
    "    def __init__(self):\n",
    "        self.counts = {}\n",
    "        self.wordlist = []\n",
    "        \n",
    "    def add_words(self, key, new_words):\n",
    "        if key not in self.counts.keys():\n",
    "            self.counts[key] = dict([(word,0) for word in self.wordlist])\n",
    "        \n",
    "        for word in new_words:\n",
    "            if word not in self.wordlist:\n",
    "                self.__update_wordlist__(word)\n",
    "                \n",
    "            self.counts[key][word] += 1\n",
    "            \n",
    "    def __update_wordlist__(self, word):\n",
    "        self.wordlist.append(word)\n",
    "        for key in self.counts.keys():\n",
    "            self.counts[key][word] = 0\n",
    "            \n",
    "    def generate_tfidf(self):\n",
    "        #tf-idf(w, d) = bow(w, d) * log (N / # documents in which word w appears)\n",
    "        #generate dict of scaling factors for each word\n",
    "        scaling_factors = {}\n",
    "        n = len(self.counts.keys())\n",
    "        for word in self.wordlist:\n",
    "            doc_count = sum([1 for k in self.counts.keys() if self.counts[k][word] > 0]) \n",
    "            scaling_factors[word] = math.log(n / (doc_count+1))\n",
    "        \n",
    "        #generate weighted counts\n",
    "        tfidf = {}\n",
    "        for k in self.counts.keys():\n",
    "            tfidf[k] = dict([(word, str(self.counts[k][word] * scaling_factors[word])) for word in self.wordlist])\n",
    "            \n",
    "        return tfidf\n",
    "    \n",
    "    def generate_lbow(self):\n",
    "        lbow = {}\n",
    "        for k in self.counts.keys():\n",
    "            lbow[k] = dict([(word, str(math.log(self.counts[k][word]+0.000000001))) for word in self.wordlist])\n",
    "            \n",
    "        return lbow\n",
    "        \n",
    "#for file in data folder\n",
    "words = WordFeatures()\n",
    "filterlist = []\n",
    "\n",
    "for file in os.listdir(\"comments\"):\n",
    "    \n",
    "    accumulator = []\n",
    "    \n",
    "    with open(\"comments/\"+file, \"r\") as fh:\n",
    "        for line in fh:\n",
    "            \n",
    "            if line.strip() == \"*BREAK*\":\n",
    "                continue\n",
    "\n",
    "            accumulator += tokenize(line)\n",
    "        \n",
    "        name = file[:-4]\n",
    "        if len(accumulator) < 1000:\n",
    "            filterlist.append(name)\n",
    "        words.add_words(name, accumulator)\n",
    "\n",
    "#populate processed csv\n",
    "#write out tf-idf values and OCEAN values to new csv\n",
    "csv = load_csv(\"big5.csv\")\n",
    "#tfidf = words.generate_tfidf()\n",
    "tfidf = words.generate_lbow()\n",
    "\n",
    "for i in range(len(csv)): #for each row\n",
    "    for word in words.wordlist:\n",
    "        if i == 0:\n",
    "            csv[i].append(word)\n",
    "        else:\n",
    "            name = csv[i][0]\n",
    "            try:\n",
    "                csv[i].append(tfidf[name][word])\n",
    "            except:\n",
    "                filterlist.append(name)\n",
    "                break\n",
    "\n",
    "with open(\"processed_big5.csv\", \"w\") as fh:\n",
    "    for row in csv:\n",
    "        if row[0] not in filterlist:\n",
    "            print(','.join(row), file=fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ridge regression, SVR, or regression tree\n",
    "\n",
    "from sklearn import linear_model\n",
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "#from sklearn.svm import SVR\n",
    "#from sklearn.kernel_ridge import KernelRidge\n",
    "import pickle\n",
    "\n",
    "data = load_csv(\"processed_big5.csv\")\n",
    "\n",
    "#extract out inputs and outputs\n",
    "split_index = int(0.8 * len(data))\n",
    "train_x = [[float(i) for i in row[6:]] for row in data[1:split_index]]\n",
    "test_x = [[float(i) for i in row[6:]] for row in data[split_index:]]\n",
    "\n",
    "outputs = {}\n",
    "for i in range(1,6):\n",
    "    key = data[0][i]\n",
    "    y_data = [row[i] for row in data]\n",
    "    y_train = [float(i) for i in y_data[1:split_index]]\n",
    "    y_test = [float(i) for i in y_data[split_index:]]\n",
    "    \n",
    "    outputs[key] = {\"train\":y_train, \"test\":y_test}\n",
    "\n",
    "#build and evaluate models\n",
    "for Y_key in outputs.keys():\n",
    "    \n",
    "    model = linear_model.Ridge(alpha=0.25)\n",
    "    #model = linear_model.Lasso(alpha=0.5)\n",
    "    #model = RandomForestRegressor(max_depth=5, random_state=0)\n",
    "    #model = KernelRidge(kernel=\"rbf\", gamma=0.1, alpha=0.5)\n",
    "    #model = SVR(kernel=\"poly\")\n",
    "    model.fit(train_x, outputs[Y_key][\"train\"])\n",
    "    \n",
    "    r2 = model.score(test_x, outputs[Y_key][\"test\"])\n",
    "    print(Y_key + \" r-squared: \" + str(r2))\n",
    "    \n",
    "    #pickle model\n",
    "    fh = open(Y_key+\".model\", \"wb\")\n",
    "    pickle.dump(model, fh)\n",
    "    fh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "import pickle, requests, time\n",
    "\n",
    "def retrieve_comments(username):\n",
    "    comment_text = None\n",
    "    last_id = None\n",
    "    usertext = []\n",
    "    \n",
    "    while comment_text != []:\n",
    "        if last_id == None:\n",
    "            r = requests.get(\"https://www.reddit.com/user/{}/comments.json?limit=100\".format(username), headers = {'User-agent':user_agent_string})\n",
    "        else:\n",
    "            r = requests.get(\"https://www.reddit.com/user/{}/comments.json?limit=100&after={}\".format(username, last_id), headers = {'User-agent':user_agent_string})\n",
    "\n",
    "        r_data = r.json()\n",
    "        \n",
    "        if 'error' in r_data.keys() and r_data['error'] == 403:\n",
    "            break\n",
    "        \n",
    "        till_reset = int(r.headers['x-ratelimit-reset'])\n",
    "        req_remaining = int(r.headers['x-ratelimit-remaining'])\n",
    "        \n",
    "        if req_remaining == 0:\n",
    "            time.sleep(till_reset+5)\n",
    "        \n",
    "        comment_text, last_id = get_comment_text(r_data)\n",
    "\n",
    "        for comment in comment_text:\n",
    "            comment = comment.encode(\"ascii\", errors=\"ignore\").decode()\n",
    "            comment = tokenize(comment)\n",
    "            usertext = usertext + comment\n",
    "            \n",
    "        time.sleep(5)\n",
    "            \n",
    "    return usertext\n",
    "                \n",
    "#get target user\n",
    "target_user = None\n",
    "\n",
    "#get wordlist from processed_big5.csv\n",
    "csv = load_csv(\"processed_big5.csv\")\n",
    "wordlist = csv[0][6:]\n",
    "\n",
    "#retrieve user comment history\n",
    "usertext = retrieve_comments(target_user)\n",
    "\n",
    "#process and count words from comment history\n",
    "#the tf-idf weighting scheme seems like it won't work one single document corpuses\n",
    "counts = [[math.log(usertext.count(word)+0.000000001) for word in wordlist]]\n",
    "\n",
    "#load models\n",
    "with open(\"O.model\", \"rb\") as fh:\n",
    "    O_model = pickle.load(fh)\n",
    "with open(\"C.model\", \"rb\") as fh:\n",
    "    C_model = pickle.load(fh)\n",
    "with open(\"E.model\", \"rb\") as fh:\n",
    "    E_model = pickle.load(fh)\n",
    "with open(\"A.model\", \"rb\") as fh:\n",
    "    A_model = pickle.load(fh)\n",
    "with open(\"N.model\", \"rb\") as fh:\n",
    "    N_model = pickle.load(fh)\n",
    "\n",
    "#run model.predict() for each model\n",
    "print(\"O: {}\".format(O_model.predict(counts)))\n",
    "print(\"C: {}\".format(C_model.predict(counts)))\n",
    "print(\"E: {}\".format(E_model.predict(counts)))\n",
    "print(\"A: {}\".format(A_model.predict(counts)))\n",
    "print(\"N: {}\".format(N_model.predict(counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
